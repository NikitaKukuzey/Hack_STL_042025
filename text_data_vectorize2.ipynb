{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab3de33f-817c-4d18-8804-32cb358493af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import gc\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "from transliterate import translit\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dcdefa3-e52a-4709-b12a-1efe5bf55f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_word_count(s: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse a 'word_count' string into a dictionary of {word: count}.\n",
    "    Performs cleaning (lowercasing, trimming punctuation) for robustness.\n",
    "    \"\"\"\n",
    "    word_dict = {}\n",
    "    if not isinstance(s, str):\n",
    "        return word_dict  # return empty for non-string input\n",
    "    \n",
    "    # Split by whitespace and parse in pairs\n",
    "    tokens = s.strip().split()\n",
    "    if not tokens:\n",
    "        return word_dict\n",
    "    # Ensure even number of tokens (word, count pairs)\n",
    "    if len(tokens) % 2 != 0:\n",
    "        tokens = tokens[:-1]  # drop any odd trailing token\n",
    "    \n",
    "    # Iterate over word/count pairs\n",
    "    for i in range(0, len(tokens), 2):\n",
    "        word = tokens[i]\n",
    "        count_str = tokens[i+1] if i+1 < len(tokens) else \"0\"\n",
    "        # Clean the word: lowercase and strip punctuation at edges\n",
    "        word_clean = word.lower()\n",
    "        word_clean = re.sub(r'^[\\W_]+|[\\W_]+$', '', word_clean)  # remove non-alphanumeric underscores at ends\n",
    "        if word_clean == \"\" or word_clean.isdigit():\n",
    "            # Skip purely numeric or empty tokens (considered noise or already handled as counts)\n",
    "            continue\n",
    "        try:\n",
    "            count = int(count_str)\n",
    "        except:\n",
    "            # If count is not a valid integer, skip this pair\n",
    "            continue\n",
    "        if count <= 0:\n",
    "            continue\n",
    "        # Accumulate counts for the word (merge duplicates if any)\n",
    "        if word_clean in word_dict:\n",
    "            word_dict[word_clean] += count\n",
    "        else:\n",
    "            word_dict[word_clean] = count\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c84d33d5-121d-4bc4-af22-9f670b212d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Читаем TSV\n",
    "df = #joblib.load('ptext_v1.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7d7a854-3ee2-45fa-aead-9196ecbbf75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices    = df['index'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59e5ac30-0f8c-4c2a-b644-2ebeef68cfca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_indices.joblib']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(indices, 'text_indices.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c515021b-5769-48ef-9bd2-85f59767d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def dict_generator():\n",
    "    for s in df['word_count'].fillna(''):\n",
    "        yield parse_word_count(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74523d69-2c83-4a0b-989f-e14410e67696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(274446, 64993)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "gc.collect()\n",
    "# 3) DictVectorizer → матрица счётчиков (sparse)\n",
    "dv = DictVectorizer(sparse=True,dtype=np.int32)\n",
    "X_counts = dv.fit_transform(dict_generator())\n",
    "\n",
    "print(X_counts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19103fa2-f1b4-4925-a778-3de86e23493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_names = dv.get_feature_names_out()\n",
    "#np.save(\"feature_names\",feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32948b06-8755-48eb-a1db-b04c606c054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse.save_npz('X_counts.npz', X_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2e39370-660b-4382-a189-45497ae2f35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "# 4) TF–IDF\n",
    "tfidf = TfidfTransformer(norm=None, use_idf=True) #не нормируем тк нормируем в конце\n",
    "X_tfidf = tfidf.fit_transform(X_counts)    # shape = [n_docs, V]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46e7a9aa-7c8f-4d6a-bb2d-8c2ed4785310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# Сохраняем в .npz формате\n",
    "#sparse.save_npz('X_tfidf.npz', X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb220d1d-7657-443f-a93f-93e504d21882",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True) #не нормируем тк нормируем в конце\n",
    "X_tfidf = tfidf.fit_transform(X_counts)    # shape = [n_docs, V]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9dc6a036-ce15-4888-ba94-098a29b936ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse.save_npz('X_tfidfnorm.npz', X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16ae7bf5-78b6-4d67-9213-26ea5aee20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = sparse.load_npz('X_tfidf.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe96e97c-32a3-4ba7-9285-b6556ed45728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(274446, 64993)\n"
     ]
    }
   ],
   "source": [
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c1011a7-8900-4d68-9afb-e077414e5065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Загрузка предобученных бинарных моделей (они 300‑мерные)\n",
    "en_model = fasttext.load_model('cc.en.300.bin')\n",
    "ru_model = fasttext.load_model('cc.ru.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "66b127fe-483a-4c28-b092-2795b83ccb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Загружаем матрицы выравнивания https://github.com/babylonhealth/fastText_multilingual\n",
    "A_en = np.loadtxt('alignment_matrices/en.txt')  # shape [300,300]\n",
    "A_ru = np.loadtxt('alignment_matrices/ru.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "28286cae-71b0-4eb8-aa63-eb0587b8b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Теперь для каждого слова — только одна проекция\n",
    "all_words  = np.load('feature_names.npy',allow_pickle=True) #dv.get_feature_names_out() #np.load('fature_names123.npy',allow_pickle=True)\n",
    "V = len(all_words)\n",
    "all_reduced = np.empty((V, 300))\n",
    "\n",
    "en_vocab = set(en_model.words)\n",
    "for i, w in enumerate(all_words):\n",
    "    # 300‑мерный fastText‑вектор\n",
    "    if w in en_vocab:\n",
    "        v300 = en_model.get_word_vector(w)\n",
    "        all_reduced[i] = v300.dot(A_en)\n",
    "    else:\n",
    "        v300 = ru_model.get_word_vector(w)\n",
    "        all_reduced[i] = v300.dot(A_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2adb9b9a-9edd-423f-82a3-85374e043f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 4) Собираем «словарь» векторов для всех фичей сразу (опционально для ускорения)\n",
    "E = all_reduced\n",
    "\n",
    "del all_reduced, en_model, ru_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5971d4fd-8b7b-4656-b6ce-0d6fe6906204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64993, 300)\n"
     ]
    }
   ],
   "source": [
    "print(E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3dffe84d-5baf-442c-a612-c7fea1e4093c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "doc_emb = X_tfidf.dot(E)\n",
    "doc_emb = normalize(doc_emb, norm='l2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "128a4f5c-af3d-4011-b874-28a50e611dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(274446, 300)\n",
      "(274446, 64993)\n"
     ]
    }
   ],
   "source": [
    "print(doc_emb.shape)\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c36316f-5fab-4dde-b884-d9877454c220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           emb_0     emb_1     emb_2     emb_3     emb_4     emb_5     emb_6  \\\n",
      "81619   0.026933 -0.002485 -0.052109 -0.027761  0.032831  0.058368 -0.004885   \n",
      "288108 -0.002521  0.031942 -0.024314  0.038365  0.041401  0.083446  0.024981   \n",
      "252671 -0.015113 -0.022096 -0.038125 -0.017852  0.035653  0.026642  0.033787   \n",
      "123476 -0.012253 -0.086710 -0.004167 -0.021619  0.025792  0.032874 -0.052965   \n",
      "13202   0.020410  0.024960 -0.019017 -0.000660  0.027122  0.071049  0.000223   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "24979   0.006815  0.057387 -0.075756 -0.012949  0.008710  0.023851  0.039128   \n",
      "336266 -0.025174  0.032423 -0.047796 -0.037746  0.013046  0.017145  0.030569   \n",
      "343454  0.018964 -0.018258 -0.051779 -0.068239  0.015676  0.000222 -0.020930   \n",
      "123422 -0.000439  0.041426 -0.022043 -0.000849  0.008002  0.026170  0.008387   \n",
      "23885  -0.005598  0.013536 -0.022707  0.059082  0.057551  0.042931  0.044471   \n",
      "\n",
      "           emb_7     emb_8     emb_9  ...   emb_290   emb_291   emb_292  \\\n",
      "81619   0.078870  0.046873 -0.046174  ...  0.008666 -0.052435 -0.030918   \n",
      "288108 -0.012471  0.010377  0.011734  ...  0.031913  0.084380  0.040082   \n",
      "252671 -0.037593  0.073828 -0.013513  ...  0.081499  0.028930  0.019452   \n",
      "123476 -0.037119  0.107428 -0.065755  ...  0.046363  0.068458 -0.051941   \n",
      "13202  -0.021329  0.020271  0.042897  ...  0.041921  0.031022  0.052346   \n",
      "...          ...       ...       ...  ...       ...       ...       ...   \n",
      "24979   0.000619  0.035887 -0.037271  ...  0.070899  0.086158 -0.008128   \n",
      "336266 -0.020739  0.057325 -0.026617  ...  0.105741  0.092462  0.004383   \n",
      "343454 -0.008037  0.061942  0.022323  ...  0.037904  0.020547  0.063980   \n",
      "123422 -0.045518  0.030803 -0.031380  ...  0.100172  0.070963 -0.001468   \n",
      "23885  -0.073924  0.146284 -0.104067  ...  0.054390 -0.010664 -0.092555   \n",
      "\n",
      "         emb_293   emb_294   emb_295   emb_296   emb_297   emb_298   emb_299  \n",
      "81619  -0.030529  0.077933 -0.028581  0.075291  0.109352  0.009187 -0.106196  \n",
      "288108 -0.057791 -0.048152 -0.042735 -0.015757  0.208977  0.027508 -0.138700  \n",
      "252671 -0.073847  0.019720 -0.040874 -0.014452  0.079120 -0.004898 -0.078159  \n",
      "123476 -0.090305 -0.007099 -0.020373  0.059858  0.147762 -0.022954 -0.094408  \n",
      "13202  -0.014810  0.038023 -0.004236  0.073535  0.139234 -0.007404 -0.114438  \n",
      "...          ...       ...       ...       ...       ...       ...       ...  \n",
      "24979  -0.040166  0.003131 -0.015915  0.001702  0.070296 -0.004000 -0.092533  \n",
      "336266 -0.041458  0.029768  0.003315  0.015983  0.085445  0.002323 -0.095511  \n",
      "343454 -0.053487  0.003271 -0.028547  0.053158  0.145165  0.027972 -0.100562  \n",
      "123422 -0.069463 -0.007317 -0.030704  0.016088  0.154724  0.013600 -0.071307  \n",
      "23885  -0.072228 -0.031626 -0.043405  0.013145  0.006529 -0.034529 -0.088467  \n",
      "\n",
      "[274446 rows x 300 columns]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "col_names = [f'emb_{i}' for i in range(doc_emb.shape[1])]\n",
    "indices = joblib.load('text_indices.joblib') \n",
    "df_emb = pd.DataFrame(doc_emb, index=indices, columns=col_names)\n",
    "print(df_emb)\n",
    "# 4) Сохраняем в CSV (можно добавить compression='gzip' при необходимости)\n",
    "df_emb.to_csv('doc_emb_norm_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1c4028a8-6820-4ad6-a6e3-50b1420bdcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA-признаки сохранены в text_data_lsa.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k = 300 # Количество признаков после svd(сингулярного разложения)\n",
    "# 5) Truncated SVD → LSA‑пространство размерности k\n",
    "svd = TruncatedSVD(n_components=k, random_state=42)\n",
    "X_lsa = svd.fit_transform(X_tfidf)         # shape = [n_docs, k]\n",
    "\n",
    "# 6) Собираем DataFrame с индексами и LSA‑фичами\n",
    "lsa_cols = [f'lsa_{i+1}' for i in range(k)]\n",
    "out_df = pd.DataFrame(X_lsa, columns=lsa_cols, index=indices)\n",
    "out_df.index.name = 'index'\n",
    "out_df.reset_index(inplace=True)\n",
    "\n",
    "# 7) Сохраняем результат\n",
    "out_df.to_csv(\"text_data_lsa300.csv\", index=False)\n",
    "print(f\"LSA-признаки сохранены в text_data_lsa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "05b56b4f-4909-4277-a2f2-aa4aba3678f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA-признаки сохранены в text_data_lsa.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k = 200 # Количество признаков после svd(сингулярного разложения)\n",
    "# 5) Truncated SVD → LSA‑пространство размерности k\n",
    "svd = TruncatedSVD(n_components=k, random_state=42)\n",
    "X_lsa = svd.fit_transform(X_tfidf)         # shape = [n_docs, k]\n",
    "\n",
    "# 6) Собираем DataFrame с индексами и LSA‑фичами\n",
    "lsa_cols = [f'lsa_{i+1}' for i in range(k)]\n",
    "out_df = pd.DataFrame(X_lsa, columns=lsa_cols, index=indices)\n",
    "out_df.index.name = 'index'\n",
    "out_df.reset_index(inplace=True)\n",
    "\n",
    "# 7) Сохраняем результат\n",
    "out_df.to_csv(\"text_data_lsa200.csv\", index=False)\n",
    "print(f\"LSA-признаки сохранены в text_data_lsa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f70738-8831-4f81-99ec-5c2e7e713ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec9e7fe-fb22-4aca-9e36-828100ae596d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d9c1e8-8542-427f-bd18-9a8f8dc81aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f071a83f-5cff-4d59-9898-a7e4f1f85232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
